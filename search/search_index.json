{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Yanagishima Yanagishima is an open-source Web application for Presto, Hive, Elasticsearch and Spark. Features Easy to install Easy to use Run query (Ctrl+Enter) Query history Query bookmark Show query execution list Kill running query Show columns Show partitions Show query result data size Show query result line number TSV download CSV download Show Presto view ddl Share query Share query result Search table(Presto only) Handle multiple Presto/Hive clusters Auto detection of partition key Show progress of running query Query parameters substitution Insert chart Format query (Ctrl+Shift+F, Presto only) Convert from TSV to values query(Presto only) Function, table completion (Ctrl+Space, Presto only) Validation (Shift+Enter, Presto only) Export/import history Export/import bookmark Desktop notification (HTTPS only) Pretty print for json/map data Enable to compare query result Comment about query Convert Hive/Presto query Support graphviz to visualize Presto explain result Support Elasticsearch SQL Label Pivot Support Spark SQL Show stats for Presto","title":"Overview"},{"location":"#welcome-to-yanagishima","text":"Yanagishima is an open-source Web application for Presto, Hive, Elasticsearch and Spark.","title":"Welcome to Yanagishima"},{"location":"#features","text":"Easy to install Easy to use Run query (Ctrl+Enter) Query history Query bookmark Show query execution list Kill running query Show columns Show partitions Show query result data size Show query result line number TSV download CSV download Show Presto view ddl Share query Share query result Search table(Presto only) Handle multiple Presto/Hive clusters Auto detection of partition key Show progress of running query Query parameters substitution Insert chart Format query (Ctrl+Shift+F, Presto only) Convert from TSV to values query(Presto only) Function, table completion (Ctrl+Space, Presto only) Validation (Shift+Enter, Presto only) Export/import history Export/import bookmark Desktop notification (HTTPS only) Pretty print for json/map data Enable to compare query result Comment about query Convert Hive/Presto query Support graphviz to visualize Presto explain result Support Elasticsearch SQL Label Pivot Support Spark SQL Show stats for Presto","title":"Features"},{"location":"config/","text":"Configuration You need to edit conf/yanagishima.properties file. # yanagishima web port jetty.port=8080 # 30 minutes. If presto query exceeds this time, yanagishima cancel the query. presto.query.max-run-time-seconds=1800 # 1GB. If presto query result file size exceeds this value, yanagishima cancel the query. presto.max-result-file-byte-size=1073741824 # you can specify freely. But you need to specify same name to presto.coordinator.server.[...] and presto.redirect.server.[...] and catalog.[...] and schema.[...] presto.datasources=your-presto # presto coordinator url presto.coordinator.server.your-presto=http://presto.coordinator:8080 # almost same as presto coordinator url. If you use reverse proxy, specify it presto.redirect.server.your-presto=http://presto.coordinator:8080 # presto catalog name catalog.your-presto=hive # presto schema name schema.your-presto=default # presto user user.your-presto=yanagishima # presto source source.your-presto=yanagishima # if query result exceeds this limit, to show rest of result is skipped select.limit=500 # http header name for audit log audit.http.header.name=some.auth.header # limit to convert from tsv to values query to.values.query.limit=500 # authorization feature check.datasource=false hive.jdbc.url.your-hive=jdbc:hive2://localhost:10000/default;auth=noSasl hive.jdbc.user.your-hive=yanagishima-hive hive.jdbc.password.your-hive=yanagishima-hive hive.query.max-run-time-seconds=3600 hive.query.max-run-time-seconds.your-hive=3600 resource.manager.url.your-hive=http://localhost:8088 sql.query.engines=presto,hive hive.datasources=your-hive hive.disallowed.keywords.your-hive=insert,drop # 1GB. If hive query result file size exceeds this value, yanagishima cancel the query. hive.max-result-file-byte-size=1073741824 # setup initial hive query(for example, set hive.mapred.mode=strict) hive.setup.query.path.your-hive=/usr/local/yanagishima/conf/hive_setup_query_your-hive # CORS setting cors.enabled=false Single Presto cluster jetty.port=8080 presto.datasources=your-presto presto.coordinator.server.your-presto=http://presto.coordinator:8080 catalog.your-presto=hive schema.your-presto=default sql.query.engines=presto Multiple Presto clusters jetty.port=8080 presto.datasources=presto1,presto2 presto.coordinator.server.presto1=http://presto1.coordinator:8080 presto.coordinator.server.presto2=http://presto2.coordinator:8080 catalog.presto1=hive schema.presto1=default catalog.presto2=hive schema.presto2=default sql.query.engines=presto Single Hive cluster jetty.port=8080 hive.jdbc.url.your-hive=jdbc:hive2://localhost:10000/default;auth=noSasl hive.jdbc.user.your-hive=yanagishima-hive hive.jdbc.password.your-hive=yanagishima-hive resource.manager.url.your-hive=http://localhost:8088 sql.query.engines=hive hive.datasources=your-hive Kerberized Hive cluster jetty.port=8080 hive.jdbc.url.your-hive=jdbc:hive2://localhost:10000/default;auth=noSasl hive.jdbc.user.your-hive=yanagishima-hive hive.jdbc.password.your-hive=yanagishima-hive resource.manager.url.your-hive=http://localhost:8088 sql.query.engines=hive hive.datasources=your-hive use.jdbc.cancel.your-hive=true Presto and Hive jetty.port=8080 presto.datasources=your-cluster presto.coordinator.server.your-cluster=http://presto.coordinator:8080 catalog.your-cluster=hive schema.your-cluster=default hive.jdbc.url.your-cluster=jdbc:hive2://localhost:10000/default;auth=noSasl hive.jdbc.user.your-cluster=yanagishima-hive hive.jdbc.password.your-cluster=yanagishima-hive resource.manager.url.your-cluster=http://localhost:8088 sql.query.engines=presto,hive hive.datasources=your-cluster Elasticsearch jetty.port=8080 elasticsearch.jdbc.url.your-elasticsearch=jdbc:es:localhost:9200 elasticsearch.datasources=your-elasticsearch sql.query.engines=elasticsearch Spark jetty.port=8080 spark.jdbc.url.your-spark=jdbc:hive2://sparkthriftserver:10000 spark.web.url.your-spark=http://sparkthriftserver:4040 resource.manager.url.your-hive=http://localhost:8088 sql.query.engines=spark spark.datasources=your-spark","title":"Configuration"},{"location":"config/#configuration","text":"You need to edit conf/yanagishima.properties file. # yanagishima web port jetty.port=8080 # 30 minutes. If presto query exceeds this time, yanagishima cancel the query. presto.query.max-run-time-seconds=1800 # 1GB. If presto query result file size exceeds this value, yanagishima cancel the query. presto.max-result-file-byte-size=1073741824 # you can specify freely. But you need to specify same name to presto.coordinator.server.[...] and presto.redirect.server.[...] and catalog.[...] and schema.[...] presto.datasources=your-presto # presto coordinator url presto.coordinator.server.your-presto=http://presto.coordinator:8080 # almost same as presto coordinator url. If you use reverse proxy, specify it presto.redirect.server.your-presto=http://presto.coordinator:8080 # presto catalog name catalog.your-presto=hive # presto schema name schema.your-presto=default # presto user user.your-presto=yanagishima # presto source source.your-presto=yanagishima # if query result exceeds this limit, to show rest of result is skipped select.limit=500 # http header name for audit log audit.http.header.name=some.auth.header # limit to convert from tsv to values query to.values.query.limit=500 # authorization feature check.datasource=false hive.jdbc.url.your-hive=jdbc:hive2://localhost:10000/default;auth=noSasl hive.jdbc.user.your-hive=yanagishima-hive hive.jdbc.password.your-hive=yanagishima-hive hive.query.max-run-time-seconds=3600 hive.query.max-run-time-seconds.your-hive=3600 resource.manager.url.your-hive=http://localhost:8088 sql.query.engines=presto,hive hive.datasources=your-hive hive.disallowed.keywords.your-hive=insert,drop # 1GB. If hive query result file size exceeds this value, yanagishima cancel the query. hive.max-result-file-byte-size=1073741824 # setup initial hive query(for example, set hive.mapred.mode=strict) hive.setup.query.path.your-hive=/usr/local/yanagishima/conf/hive_setup_query_your-hive # CORS setting cors.enabled=false","title":"Configuration"},{"location":"config/#single-presto-cluster","text":"jetty.port=8080 presto.datasources=your-presto presto.coordinator.server.your-presto=http://presto.coordinator:8080 catalog.your-presto=hive schema.your-presto=default sql.query.engines=presto","title":"Single Presto cluster"},{"location":"config/#multiple-presto-clusters","text":"jetty.port=8080 presto.datasources=presto1,presto2 presto.coordinator.server.presto1=http://presto1.coordinator:8080 presto.coordinator.server.presto2=http://presto2.coordinator:8080 catalog.presto1=hive schema.presto1=default catalog.presto2=hive schema.presto2=default sql.query.engines=presto","title":"Multiple Presto clusters"},{"location":"config/#single-hive-cluster","text":"jetty.port=8080 hive.jdbc.url.your-hive=jdbc:hive2://localhost:10000/default;auth=noSasl hive.jdbc.user.your-hive=yanagishima-hive hive.jdbc.password.your-hive=yanagishima-hive resource.manager.url.your-hive=http://localhost:8088 sql.query.engines=hive hive.datasources=your-hive","title":"Single Hive cluster"},{"location":"config/#kerberized-hive-cluster","text":"jetty.port=8080 hive.jdbc.url.your-hive=jdbc:hive2://localhost:10000/default;auth=noSasl hive.jdbc.user.your-hive=yanagishima-hive hive.jdbc.password.your-hive=yanagishima-hive resource.manager.url.your-hive=http://localhost:8088 sql.query.engines=hive hive.datasources=your-hive use.jdbc.cancel.your-hive=true","title":"Kerberized Hive cluster"},{"location":"config/#presto-and-hive","text":"jetty.port=8080 presto.datasources=your-cluster presto.coordinator.server.your-cluster=http://presto.coordinator:8080 catalog.your-cluster=hive schema.your-cluster=default hive.jdbc.url.your-cluster=jdbc:hive2://localhost:10000/default;auth=noSasl hive.jdbc.user.your-cluster=yanagishima-hive hive.jdbc.password.your-cluster=yanagishima-hive resource.manager.url.your-cluster=http://localhost:8088 sql.query.engines=presto,hive hive.datasources=your-cluster","title":"Presto and Hive"},{"location":"config/#elasticsearch","text":"jetty.port=8080 elasticsearch.jdbc.url.your-elasticsearch=jdbc:es:localhost:9200 elasticsearch.datasources=your-elasticsearch sql.query.engines=elasticsearch","title":"Elasticsearch"},{"location":"config/#spark","text":"jetty.port=8080 spark.jdbc.url.your-spark=jdbc:hive2://sparkthriftserver:10000 spark.web.url.your-spark=http://sparkthriftserver:4040 resource.manager.url.your-hive=http://localhost:8088 sql.query.engines=spark spark.datasources=your-spark","title":"Spark"},{"location":"deployment/","text":"Deployment Production Highly recommend to deploy in HTTPS due to security, clipboard copy, desktop notification Authentication and authorization yanagishima doesn't have authentication/authorization feature. But, if you have any reverse proxy server for yanagishima and that reverse proxy server provides HTTP level authentication, you can use it for yanagishima too. yanagishima can log username for each query executions and authorize per datasource. If your reverse proxy server sets username on HTTP header just after authentication, before proxied requests you can use it. In this case, please specify audit.http.header.name which is http header name to be passed through your proxy. If you want to deny to access without username, please specify user.require=true If you set check.datasource=true and datasource list which you want to allow on HTTP header X-yanagishima-datasources through your proxy, authorization feature is enabled. For example, if there are three datasources(aaa and bbb and ccc) and X-yanagishima-datasources=aaa,bbb is set, user can't access to datasource ccc. If you use a presto with LDAP, you need to specify auth.xxx=true in your yanagishima.properties jetty.port=8080 presto.datasources=your-presto presto.coordinator.server.your-presto=http://presto.coordinator:8080 catalog.your-presto=hive schema.your-presto=default sql.query.engines=presto auth.your-presto=true","title":"Deployment"},{"location":"deployment/#deployment","text":"","title":"Deployment"},{"location":"deployment/#production","text":"Highly recommend to deploy in HTTPS due to security, clipboard copy, desktop notification","title":"Production"},{"location":"deployment/#authentication-and-authorization","text":"yanagishima doesn't have authentication/authorization feature. But, if you have any reverse proxy server for yanagishima and that reverse proxy server provides HTTP level authentication, you can use it for yanagishima too. yanagishima can log username for each query executions and authorize per datasource. If your reverse proxy server sets username on HTTP header just after authentication, before proxied requests you can use it. In this case, please specify audit.http.header.name which is http header name to be passed through your proxy. If you want to deny to access without username, please specify user.require=true If you set check.datasource=true and datasource list which you want to allow on HTTP header X-yanagishima-datasources through your proxy, authorization feature is enabled. For example, if there are three datasources(aaa and bbb and ccc) and X-yanagishima-datasources=aaa,bbb is set, user can't access to datasource ccc. If you use a presto with LDAP, you need to specify auth.xxx=true in your yanagishima.properties jetty.port=8080 presto.datasources=your-presto presto.coordinator.server.your-presto=http://presto.coordinator:8080 catalog.your-presto=hive schema.your-presto=default sql.query.engines=presto auth.your-presto=true","title":"Authentication and authorization"},{"location":"developers/","text":"Developers Backend Framework Java 11 Lombock Build ./gradlew clean build Frontend File Description Copy to docroot Build index.js index.html Mount point for Vue Yes static/favicon.ico Favorite icon Yes src Source files Yes src/main.js Entry point Yes src/App.vue Root component Yes src/components Vue components Yes src/router Vue Router routes Yes src/store Vuex store Yes src/views Views which are switched by Vue Router Yes src/assets/yanagishima.svg Logo/Background image Yes src/assets/scss/bootstrap.scss CSS based on Bootstrap Yes build Build scripts for webpack - - config Build configs for webpack - - Framework CSS Bootstrap 4.1.3 FontAwesome 5.3.1 Google Fonts \" Droid+Sans \" JavaScript Vue 2.5.2 Vuex 3.0.1 Vue Router 3.0.1 Ace Editor 1.3.3 Sugar 2.0.4 jQuery 3.3.1 Build/Serve tool webpack 3.6.0 Customization Dependence Xcode Node.js Install dependencies $ cd web $ npm install Build $ npm run build Build/Serve and Livereload $ npm start","title":"Developers"},{"location":"developers/#developers","text":"","title":"Developers"},{"location":"developers/#backend","text":"","title":"Backend"},{"location":"developers/#framework","text":"Java 11 Lombock","title":"Framework"},{"location":"developers/#build","text":"./gradlew clean build","title":"Build"},{"location":"developers/#frontend","text":"File Description Copy to docroot Build index.js index.html Mount point for Vue Yes static/favicon.ico Favorite icon Yes src Source files Yes src/main.js Entry point Yes src/App.vue Root component Yes src/components Vue components Yes src/router Vue Router routes Yes src/store Vuex store Yes src/views Views which are switched by Vue Router Yes src/assets/yanagishima.svg Logo/Background image Yes src/assets/scss/bootstrap.scss CSS based on Bootstrap Yes build Build scripts for webpack - - config Build configs for webpack - -","title":"Frontend"},{"location":"developers/#framework_1","text":"CSS Bootstrap 4.1.3 FontAwesome 5.3.1 Google Fonts \" Droid+Sans \" JavaScript Vue 2.5.2 Vuex 3.0.1 Vue Router 3.0.1 Ace Editor 1.3.3 Sugar 2.0.4 jQuery 3.3.1 Build/Serve tool webpack 3.6.0","title":"Framework"},{"location":"developers/#customization","text":"","title":"Customization"},{"location":"developers/#dependence","text":"Xcode Node.js","title":"Dependence"},{"location":"developers/#install-dependencies","text":"$ cd web $ npm install","title":"Install dependencies"},{"location":"developers/#build_1","text":"$ npm run build","title":"Build"},{"location":"developers/#buildserve-and-livereload","text":"$ npm start","title":"Build/Serve and Livereload"},{"location":"release/","text":"Release Notes Version 22.0 Avoid scroll bar from hiding error message Remove embedded old bootstrap Add tooltip for presto row column type Delete unnecessary scss import Introduce starred schema Create URL to set bookmarked query with parameter Bundle ace editor Lazy load big and rarely used js libraries Redesign treeview layout Redesign Partition Layout Support presto date type Add presto query page link to query Add filter query list by source Fix bug that history tab doesn't show when you use elasticsearch Improve result tab download UI Improve Header transition Refactor server side logic Upgrade Gradle from 5.0 to 5.2 Improve server side logging logic Fix bug when user submits very long query Introduce mkdocs documentation Version 21.0 Add datasource color Add announce/notification feature Refactor UI Add detail query error message and semanticErrorName presto#790 Handle old presto due to presto#224 Improve message when result file is not found, allow.other.read.result.xxx=false Use JDK11, Gradle5 Upgrade presto client library Enable to set datetime format pattern per datasource in treeview Show more information like execution time, file size, etc when query result file is removed Fix header layout Update config add user and source (#56) Avoid scroll bar from hiding note Improve partition column fetch logic in treeview Add webhdfs.proxy.user and webhdfs.proxy.password option Add a script to launch yanagishima in foreground process (#58) Support mysql as yanagishima backend RDBMS yanagishima setting is the following database.type=mysql database.connection-url=jdbc:mysql://localhost:3306/yanagishima?useSSL=false database.user=... database.password=... Version 20.0 Show query diff Enable user to download without column header Version 19.0 Support Spark SQL Show stats for presto Version 18.0 Fix the bug that desktop notification doesn't work Improve catalog setting logic if there is no hive catalog Improve error logging Fix bug that java.lang.IllegalArgumentException: float is illegal Fix bug that publish doesn't work in HTTP Improve partition fetching logic with webhdfs Version 17.0 Add link which can open a schema/table in Treeview Pivot Fix bug which invisible.schema doesn't work Improve hive job handling when you create/drop table Add query id link to share page Update presto library Fix partition bug Remove kill button in hive Default value of use.new.show.partitions.xxx is true Version 16.0 Refactoring with Vuex, Vue Router, Single File Components Add BOM in CSV/TSV download files Improve to display hive map data Add pretty print on share view Enable code folding Make underscores available in placeholders Move treeview to left end tab Version 15.0 Support Elasticsearch SQL Add label feature like gmail Handle presto/hive reserved keywords(e.g. group, order) Integrate metadata service Add partition filter Show column number Suppresss stacktrace Version 14.0 Fix wrong line number when result file contains a new line Metadata of 14.0 is NOT compatible with 13.0, so migration is required MigrateV14.sh is the script to migrate db file. Migration process is as follows cp data/yanagishima.db data/yanagishima.db.bak bin/migrateV14.sh data/yanagishima.db result sqlite3 data/yanagishima.db sqlite> alter table query rename to query_old; sqlite> alter table query_v14 rename to query; If you confirmed, drop table query_old It takes about 10 hours if db file is more than 500MB and result file is about 1TB vacuum and create index deq_index on query(datasource, engine, query_id) and create index deu_index on query(datasource, engine, user) may be necessary if yanagishima.db is huge Fix infinite loop bug when /queryStatus returns 500 Add kill hive query feature if you use a kerberized hadoop cluster Sort table name when you use Treeview Copy publish url to clipboard but chrome user only due to Async Clipboard API Handle presto/hive reserved keywords(e.g. group, order) Use timestamp to index.js due to cache busting Version 13.0 Improve code input performance especially when query result is huge Upgrade ace editor Add message if result count exceeds 500 Improve history tab logic when result file is removed Add sort partition feature Fix bug that 3 pane compare result display disappear Don't create fluency instance every request due to performance improvement Handle issue that presto doesn't support show paritions since 0.202 Add option to use webhdfs api when there are too many partitions Version 12.0 Convert hive/presto query Support graphviz to visualize presto explain result Add tooltip to Set in History/Bookmark tab Add new presto functions(0.196) to completion list Fix bookmark bug Fix presto authentication failed bug Version 11.0 Fix timezone bug Fix exponential notation bug Support UTF-8 encoding for CSV Version 10.0 Add timeline tab Version 9.0 Pretty print for map data Add left panel to compare query result Support presto/hive authentication with user/password If you want to use presto TLS, you need to execute keytool -import https://prestosql .io/docs/current/security/tls.html Search query history Paging query history Improve performance to write/read result file Result file format of 9.0 is tsv, prior to 9.0 is json, so migration is required MigrateV9.sh is the script to migrate result file. If migration error occur, you can check it. $ bin/migrateV9.sh result dest ... processing /path/to/yanagishima-9.0/result/your-presto/20171010/20171010_072513_02895_xxvvj.json error /path/to/yanagishima-9.0/result/your-presto/20171010/20171010_072513_02895_xxvvj.json java.lang.RuntimeException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input: expected close marker for ARRAY (from [Source: java.io.StringReader@e320068; line: 1, column: 0]) at [Source: java.io.StringReader@e320068; line: 1, column: 241] at yanagishima.migration.MigrateV9.main(MigrateV9.java:59) Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input: expected close marker for ARRAY (from [Source: java.io.StringReader@e320068; line: 1, column: 0]) at [Source: java.io.StringReader@e320068; line: 1, column: 241] at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433) at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521) at org.codehaus.jackson.impl.JsonParserMinimalBase._reportInvalidEOF(JsonParserMinimalBase.java:454) at org.codehaus.jackson.impl.JsonParserBase._handleEOF(JsonParserBase.java:473) at org.codehaus.jackson.impl.ReaderBasedParser._skipWSOrEnd(ReaderBasedParser.java:1496) at org.codehaus.jackson.impl.ReaderBasedParser.nextToken(ReaderBasedParser.java:368) at org.codehaus.jackson.map.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:211) at org.codehaus.jackson.map.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:194) at org.codehaus.jackson.map.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:30) at org.codehaus.jackson.map.ObjectMapper._readMapAndClose(ObjectMapper.java:2732) at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:1863) at yanagishima.migration.MigrateV9.main(MigrateV9.java:56) processing /path/to/yanagishima-9.0/result/your-presto/20171010/20171010_072517_02897_xxvvj.json ... Version 8.0 Pretty print for json data Store query history/bookmark to server side db, but default setting is to use local storage Improve partition display Metadata of 9.0 is NOT compatible with 7.0, so migration is required Migration process is as follows cp data/yanagishima.db data/yanagishima.db.bak sqlite3 data/yanagishima.db sqlite> create table query_new (datasource text, engine text, query_id text, fetch_result_time_string text, query_string text, user text, primary key(datasource, engine, query_id)); sqlite> insert into query_new select datasource, engine, query_id, fetch_result_time_string, query_string, null from query; sqlite> alter table query rename to query_old; sqlite> alter table query_new rename to query; sqlite> create table publish_new (publish_id text, datasource text, engine text, query_id text, user text, primary key(publish_id)); sqlite> insert into publish_new select publish_id, datasource, engine, query_id, null from publish; sqlite> alter table publish rename to publish_old; sqlite> alter table publish_new rename to publish; sqlite> create table bookmark_new (bookmark_id integer primary key autoincrement, datasource text, engine text, query text, title text, user text); sqlite> insert into bookmark_new select bookmark_id, datasource, engine, query, title, null from bookmark; sqlite> alter table bookmark rename to bookmark_old; sqlite> alter table bookmark_new rename to bookmark; If you confirmed, drop table query_old, publish_old, bookmark_old; Version 7.0 Support hive on MapReduce(yanagishima executes set mapreduce.job.name=... ) Metadata of 7.0 is NOT compatible with 6.0, so migration is required Migration process is as follows cp data/yanagishima.db data/yanagishima.db.bak sqlite3 data/yanagishima.db sqlite> create table query_new (datasource text, engine text, query_id text, fetch_result_time_string text, query_string text, primary key(datasource, engine, query_id)); sqlite> insert into query_new select datasource, 'presto', query_id, fetch_result_time_string, query_string from query; sqlite> alter table query rename to query_old; sqlite> alter table query_new rename to query; sqlite> create table publish_new (publish_id text, datasource text, engine text, query_id text, primary key(publish_id)); sqlite> insert into publish_new select publish_id, datasource, 'presto', query_id from publish; sqlite> alter table publish rename to publish_old; sqlite> alter table publish_new rename to publish; sqlite> create table bookmark_new (bookmark_id integer primary key autoincrement, datasource text, engine text, query text, title text); sqlite> insert into bookmark_new select bookmark_id, datasource, 'presto', query, title from bookmark; sqlite> alter table bookmark rename to bookmark_old; sqlite> alter table bookmark_new rename to bookmark; If you confirmed, drop table query_old, publish_old, bookmark_old; Version 6.0 Support bookmark title, so add title column to bookmark table Metadata of 6.0 is NOT compatible with 5.0, so migration is required Migration process is as follows cp data/yanagishima.db data/yanagishima.db.bak sqlite3 data/yanagishima.db sqlite> create table if not exists bookmark_new (bookmark_id integer primary key autoincrement, datasource text, query text, title text); sqlite> insert into bookmark_new select bookmark_id, datasource, query, null from bookmark; sqlite> alter table bookmark rename to bookmark_old; sqlite> alter table bookmark_new rename to bookmark; If you confirmed, drop table bookmark_old;","title":"Release Notes"},{"location":"release/#release-notes","text":"","title":"Release Notes"},{"location":"release/#version-220","text":"Avoid scroll bar from hiding error message Remove embedded old bootstrap Add tooltip for presto row column type Delete unnecessary scss import Introduce starred schema Create URL to set bookmarked query with parameter Bundle ace editor Lazy load big and rarely used js libraries Redesign treeview layout Redesign Partition Layout Support presto date type Add presto query page link to query Add filter query list by source Fix bug that history tab doesn't show when you use elasticsearch Improve result tab download UI Improve Header transition Refactor server side logic Upgrade Gradle from 5.0 to 5.2 Improve server side logging logic Fix bug when user submits very long query Introduce mkdocs documentation","title":"Version 22.0"},{"location":"release/#version-210","text":"Add datasource color Add announce/notification feature Refactor UI Add detail query error message and semanticErrorName presto#790 Handle old presto due to presto#224 Improve message when result file is not found, allow.other.read.result.xxx=false Use JDK11, Gradle5 Upgrade presto client library Enable to set datetime format pattern per datasource in treeview Show more information like execution time, file size, etc when query result file is removed Fix header layout Update config add user and source (#56) Avoid scroll bar from hiding note Improve partition column fetch logic in treeview Add webhdfs.proxy.user and webhdfs.proxy.password option Add a script to launch yanagishima in foreground process (#58) Support mysql as yanagishima backend RDBMS yanagishima setting is the following database.type=mysql database.connection-url=jdbc:mysql://localhost:3306/yanagishima?useSSL=false database.user=... database.password=...","title":"Version 21.0"},{"location":"release/#version-200","text":"Show query diff Enable user to download without column header","title":"Version 20.0"},{"location":"release/#version-190","text":"Support Spark SQL Show stats for presto","title":"Version 19.0"},{"location":"release/#version-180","text":"Fix the bug that desktop notification doesn't work Improve catalog setting logic if there is no hive catalog Improve error logging Fix bug that java.lang.IllegalArgumentException: float is illegal Fix bug that publish doesn't work in HTTP Improve partition fetching logic with webhdfs","title":"Version 18.0"},{"location":"release/#version-170","text":"Add link which can open a schema/table in Treeview Pivot Fix bug which invisible.schema doesn't work Improve hive job handling when you create/drop table Add query id link to share page Update presto library Fix partition bug Remove kill button in hive Default value of use.new.show.partitions.xxx is true","title":"Version 17.0"},{"location":"release/#version-160","text":"Refactoring with Vuex, Vue Router, Single File Components Add BOM in CSV/TSV download files Improve to display hive map data Add pretty print on share view Enable code folding Make underscores available in placeholders Move treeview to left end tab","title":"Version 16.0"},{"location":"release/#version-150","text":"Support Elasticsearch SQL Add label feature like gmail Handle presto/hive reserved keywords(e.g. group, order) Integrate metadata service Add partition filter Show column number Suppresss stacktrace","title":"Version 15.0"},{"location":"release/#version-140","text":"Fix wrong line number when result file contains a new line Metadata of 14.0 is NOT compatible with 13.0, so migration is required MigrateV14.sh is the script to migrate db file. Migration process is as follows cp data/yanagishima.db data/yanagishima.db.bak bin/migrateV14.sh data/yanagishima.db result sqlite3 data/yanagishima.db sqlite> alter table query rename to query_old; sqlite> alter table query_v14 rename to query; If you confirmed, drop table query_old It takes about 10 hours if db file is more than 500MB and result file is about 1TB vacuum and create index deq_index on query(datasource, engine, query_id) and create index deu_index on query(datasource, engine, user) may be necessary if yanagishima.db is huge Fix infinite loop bug when /queryStatus returns 500 Add kill hive query feature if you use a kerberized hadoop cluster Sort table name when you use Treeview Copy publish url to clipboard but chrome user only due to Async Clipboard API Handle presto/hive reserved keywords(e.g. group, order) Use timestamp to index.js due to cache busting","title":"Version 14.0"},{"location":"release/#version-130","text":"Improve code input performance especially when query result is huge Upgrade ace editor Add message if result count exceeds 500 Improve history tab logic when result file is removed Add sort partition feature Fix bug that 3 pane compare result display disappear Don't create fluency instance every request due to performance improvement Handle issue that presto doesn't support show paritions since 0.202 Add option to use webhdfs api when there are too many partitions","title":"Version 13.0"},{"location":"release/#version-120","text":"Convert hive/presto query Support graphviz to visualize presto explain result Add tooltip to Set in History/Bookmark tab Add new presto functions(0.196) to completion list Fix bookmark bug Fix presto authentication failed bug","title":"Version 12.0"},{"location":"release/#version-110","text":"Fix timezone bug Fix exponential notation bug Support UTF-8 encoding for CSV","title":"Version 11.0"},{"location":"release/#version-100","text":"Add timeline tab","title":"Version 10.0"},{"location":"release/#version-90","text":"Pretty print for map data Add left panel to compare query result Support presto/hive authentication with user/password If you want to use presto TLS, you need to execute keytool -import https://prestosql .io/docs/current/security/tls.html Search query history Paging query history Improve performance to write/read result file Result file format of 9.0 is tsv, prior to 9.0 is json, so migration is required MigrateV9.sh is the script to migrate result file. If migration error occur, you can check it. $ bin/migrateV9.sh result dest ... processing /path/to/yanagishima-9.0/result/your-presto/20171010/20171010_072513_02895_xxvvj.json error /path/to/yanagishima-9.0/result/your-presto/20171010/20171010_072513_02895_xxvvj.json java.lang.RuntimeException: org.codehaus.jackson.JsonParseException: Unexpected end-of-input: expected close marker for ARRAY (from [Source: java.io.StringReader@e320068; line: 1, column: 0]) at [Source: java.io.StringReader@e320068; line: 1, column: 241] at yanagishima.migration.MigrateV9.main(MigrateV9.java:59) Caused by: org.codehaus.jackson.JsonParseException: Unexpected end-of-input: expected close marker for ARRAY (from [Source: java.io.StringReader@e320068; line: 1, column: 0]) at [Source: java.io.StringReader@e320068; line: 1, column: 241] at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433) at org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521) at org.codehaus.jackson.impl.JsonParserMinimalBase._reportInvalidEOF(JsonParserMinimalBase.java:454) at org.codehaus.jackson.impl.JsonParserBase._handleEOF(JsonParserBase.java:473) at org.codehaus.jackson.impl.ReaderBasedParser._skipWSOrEnd(ReaderBasedParser.java:1496) at org.codehaus.jackson.impl.ReaderBasedParser.nextToken(ReaderBasedParser.java:368) at org.codehaus.jackson.map.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:211) at org.codehaus.jackson.map.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:194) at org.codehaus.jackson.map.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:30) at org.codehaus.jackson.map.ObjectMapper._readMapAndClose(ObjectMapper.java:2732) at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:1863) at yanagishima.migration.MigrateV9.main(MigrateV9.java:56) processing /path/to/yanagishima-9.0/result/your-presto/20171010/20171010_072517_02897_xxvvj.json ...","title":"Version 9.0"},{"location":"release/#version-80","text":"Pretty print for json data Store query history/bookmark to server side db, but default setting is to use local storage Improve partition display Metadata of 9.0 is NOT compatible with 7.0, so migration is required Migration process is as follows cp data/yanagishima.db data/yanagishima.db.bak sqlite3 data/yanagishima.db sqlite> create table query_new (datasource text, engine text, query_id text, fetch_result_time_string text, query_string text, user text, primary key(datasource, engine, query_id)); sqlite> insert into query_new select datasource, engine, query_id, fetch_result_time_string, query_string, null from query; sqlite> alter table query rename to query_old; sqlite> alter table query_new rename to query; sqlite> create table publish_new (publish_id text, datasource text, engine text, query_id text, user text, primary key(publish_id)); sqlite> insert into publish_new select publish_id, datasource, engine, query_id, null from publish; sqlite> alter table publish rename to publish_old; sqlite> alter table publish_new rename to publish; sqlite> create table bookmark_new (bookmark_id integer primary key autoincrement, datasource text, engine text, query text, title text, user text); sqlite> insert into bookmark_new select bookmark_id, datasource, engine, query, title, null from bookmark; sqlite> alter table bookmark rename to bookmark_old; sqlite> alter table bookmark_new rename to bookmark; If you confirmed, drop table query_old, publish_old, bookmark_old;","title":"Version 8.0"},{"location":"release/#version-70","text":"Support hive on MapReduce(yanagishima executes set mapreduce.job.name=... ) Metadata of 7.0 is NOT compatible with 6.0, so migration is required Migration process is as follows cp data/yanagishima.db data/yanagishima.db.bak sqlite3 data/yanagishima.db sqlite> create table query_new (datasource text, engine text, query_id text, fetch_result_time_string text, query_string text, primary key(datasource, engine, query_id)); sqlite> insert into query_new select datasource, 'presto', query_id, fetch_result_time_string, query_string from query; sqlite> alter table query rename to query_old; sqlite> alter table query_new rename to query; sqlite> create table publish_new (publish_id text, datasource text, engine text, query_id text, primary key(publish_id)); sqlite> insert into publish_new select publish_id, datasource, 'presto', query_id from publish; sqlite> alter table publish rename to publish_old; sqlite> alter table publish_new rename to publish; sqlite> create table bookmark_new (bookmark_id integer primary key autoincrement, datasource text, engine text, query text, title text); sqlite> insert into bookmark_new select bookmark_id, datasource, 'presto', query, title from bookmark; sqlite> alter table bookmark rename to bookmark_old; sqlite> alter table bookmark_new rename to bookmark; If you confirmed, drop table query_old, publish_old, bookmark_old;","title":"Version 7.0"},{"location":"release/#version-60","text":"Support bookmark title, so add title column to bookmark table Metadata of 6.0 is NOT compatible with 5.0, so migration is required Migration process is as follows cp data/yanagishima.db data/yanagishima.db.bak sqlite3 data/yanagishima.db sqlite> create table if not exists bookmark_new (bookmark_id integer primary key autoincrement, datasource text, query text, title text); sqlite> insert into bookmark_new select bookmark_id, datasource, query, null from bookmark; sqlite> alter table bookmark rename to bookmark_old; sqlite> alter table bookmark_new rename to bookmark; If you confirmed, drop table bookmark_old;","title":"Version 6.0"},{"location":"upgrade/","text":"Upgrade If you want to upgrade yanagishima from xxx to yyy , steps are as follows cd yanagishima-xxx bin/yanagishima-shutdown.sh cd .. unzip yanagishima-yyy.zip cd yanagishima-yyy mv result result_bak mv yanagishima-xxx/result . cp yanagishima-xxx/data/yanagishima.db data/ cp yanagishima-xxx/conf/yanagishima.properties conf/ bin/yanagishima-start.sh If it is necessary to migrate yanagishima.db or result file, you need to migrate.","title":"Upgrade"},{"location":"upgrade/#upgrade","text":"If you want to upgrade yanagishima from xxx to yyy , steps are as follows cd yanagishima-xxx bin/yanagishima-shutdown.sh cd .. unzip yanagishima-yyy.zip cd yanagishima-yyy mv result result_bak mv yanagishima-xxx/result . cp yanagishima-xxx/data/yanagishima.db data/ cp yanagishima-xxx/conf/yanagishima.properties conf/ bin/yanagishima-start.sh If it is necessary to migrate yanagishima.db or result file, you need to migrate.","title":"Upgrade"}]}